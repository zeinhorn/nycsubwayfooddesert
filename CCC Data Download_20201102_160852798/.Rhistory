polarized.tokens <- polarized.data %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts <- polarized.tokens %>%
count(word) %>%
arrange(-n)
polarized.counts.2 <- polarized.counts %>%
head(10)
words.whole.article <- polarized.counts.2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the Whole Article")
polar.tokens <- polarized.data %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokens2 <- polar.tokens %>%
count(word) %>%
arrange(-n) %>%
head(10)
phrases.whole.article <- polar.tokens2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Whole Article")
#the top 7 words in the first half of the article
polarized.textz <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text3 <- data.frame(text = polarized.textz,
stringsAsFactors = FALSE) %>%
head(22)
polarized.tokens2 <- polarized.text3 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.3 <- polarized.tokens2 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.first.half <- polarized.counts.3 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the First Half of Article")
polar.tokenz <- polarized.text3 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensz <- polar.tokenz %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.first.half <- polar.tokensz %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the First Half of Article")
#the top 7 words in the second half of the article
polarized.textx <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text4 <- data.frame(text = polarized.textx,
stringsAsFactors = FALSE) %>%
tail(22)
polarized.tokens3 <- polarized.text4 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.4 <- polarized.tokens3 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.second.half <- polarized.counts.4 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Number") +
xlab("Word") +
ggtitle("Words in the Second Half of Article")
polar.tokenx <- polarized.text4 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensx <- polar.tokenx %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.second.half <- polar.tokensx %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Second Half of Article")
grid.arrange(words.first.half, phrases.first.half, words.second.half, phrases.second.half,
words.whole.article, phrases.whole.article)
polarized.tokens2 <- polarized.tokens %>%
count(word) %>%
inner_join(get_sentiments("afinn")) %>%
filter(!is.na(value)) %>%
mutate(total.value = n*value) %>%
summarise(mean(total.value))
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
message = FALSE)
library(stringr)
library(magrittr)
library(ggplot2)
library(tidyverse)
library(rvest)
library(tidytext)
library(gridExtra)
library(wordcloud2)
#1.
polarized <- "https://www.pewresearch.org/politics/2014/06/12/political-polarization-in-the-american-public/"
#the top 10 words in the whole article
polarized.text <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text2 <- str_c(polarized.text,
collapse = " ")
polarized.data <- data.frame(text = polarized.text2,
stringsAsFactors = FALSE)
polarized.tokens <- polarized.data %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts <- polarized.tokens %>%
count(word) %>%
arrange(-n)
polarized.counts.2 <- polarized.counts %>%
head(10)
words.whole.article <- polarized.counts.2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the Whole Article")
polar.tokens <- polarized.data %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokens2 <- polar.tokens %>%
count(word) %>%
arrange(-n) %>%
head(10)
phrases.whole.article <- polar.tokens2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Whole Article")
#the top 7 words in the first half of the article
polarized.textz <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text3 <- data.frame(text = polarized.textz,
stringsAsFactors = FALSE) %>%
head(22)
polarized.tokens2 <- polarized.text3 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.3 <- polarized.tokens2 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.first.half <- polarized.counts.3 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the First Half of Article")
polar.tokenz <- polarized.text3 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensz <- polar.tokenz %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.first.half <- polar.tokensz %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the First Half of Article")
#the top 7 words in the second half of the article
polarized.textx <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text4 <- data.frame(text = polarized.textx,
stringsAsFactors = FALSE) %>%
tail(22)
polarized.tokens3 <- polarized.text4 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.4 <- polarized.tokens3 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.second.half <- polarized.counts.4 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Number") +
xlab("Word") +
ggtitle("Words in the Second Half of Article")
polar.tokenx <- polarized.text4 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensx <- polar.tokenx %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.second.half <- polar.tokensx %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Second Half of Article")
grid.arrange(words.first.half, phrases.first.half, words.second.half, phrases.second.half,
words.whole.article, phrases.whole.article)
polarized.counts %>%
wordcloud2()
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
message = FALSE)
knitr::opts_chunk$set(echo = TRUE,
warning = FALSE,
message = FALSE)
library(stringr)
library(magrittr)
library(ggplot2)
library(tidyverse)
library(rvest)
library(tidytext)
library(gridExtra)
library(wordcloud2)
#1.
polarized <- "https://www.pewresearch.org/politics/2014/06/12/political-polarization-in-the-american-public/"
#the top 10 words in the whole article
polarized.text <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text2 <- str_c(polarized.text,
collapse = " ")
polarized.data <- data.frame(text = polarized.text2,
stringsAsFactors = FALSE)
polarized.tokens <- polarized.data %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts <- polarized.tokens %>%
count(word) %>%
arrange(-n)
polarized.counts.2 <- polarized.counts %>%
head(10)
words.whole.article <- polarized.counts.2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the Whole Article")
polar.tokens <- polarized.data %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokens2 <- polar.tokens %>%
count(word) %>%
arrange(-n) %>%
head(10)
phrases.whole.article <- polar.tokens2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Whole Article")
#the top 7 words in the first half of the article
polarized.textz <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text3 <- data.frame(text = polarized.textz,
stringsAsFactors = FALSE) %>%
head(22)
polarized.tokens2 <- polarized.text3 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.3 <- polarized.tokens2 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.first.half <- polarized.counts.3 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the First Half of Article")
polar.tokenz <- polarized.text3 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensz <- polar.tokenz %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.first.half <- polar.tokensz %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the First Half of Article")
#the top 7 words in the second half of the article
polarized.textx <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text4 <- data.frame(text = polarized.textx,
stringsAsFactors = FALSE) %>%
tail(22)
polarized.tokens3 <- polarized.text4 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.4 <- polarized.tokens3 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.second.half <- polarized.counts.4 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Number") +
xlab("Word") +
ggtitle("Words in the Second Half of Article")
polar.tokenx <- polarized.text4 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensx <- polar.tokenx %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.second.half <- polar.tokensx %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Second Half of Article")
grid.arrange(words.first.half, phrases.first.half, words.second.half, phrases.second.half,
words.whole.article, phrases.whole.article)
library(stringr)
library(magrittr)
library(ggplot2)
library(tidyverse)
library(rvest)
library(tidytext)
library(gridExtra)
library(wordcloud2)
polarized <- "https://www.pewresearch.org/politics/2014/06/12/political-polarization-in-the-american-public/"
#the top 10 words in the whole article
polarized.text <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text2 <- str_c(polarized.text,
collapse = " ")
polarized.data <- data.frame(text = polarized.text2,
stringsAsFactors = FALSE)
polarized.tokens <- polarized.data %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts <- polarized.tokens %>%
count(word) %>%
arrange(-n)
polarized.counts.2 <- polarized.counts %>%
head(10)
words.whole.article <- polarized.counts.2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the Whole Article")
polar.tokens <- polarized.data %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokens2 <- polar.tokens %>%
count(word) %>%
arrange(-n) %>%
head(10)
phrases.whole.article <- polar.tokens2 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Whole Article")
#the top 7 words in the first half of the article
polarized.textz <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text3 <- data.frame(text = polarized.textz,
stringsAsFactors = FALSE) %>%
head(22)
polarized.tokens2 <- polarized.text3 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.3 <- polarized.tokens2 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.first.half <- polarized.counts.3 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Word") +
xlab("Number") +
ggtitle("Words in the First Half of Article")
polar.tokenz <- polarized.text3 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensz <- polar.tokenz %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.first.half <- polar.tokensz %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the First Half of Article")
#the top 7 words in the second half of the article
polarized.textx <- polarized %>%
read_html() %>%
html_nodes("p") %>%
html_text()
polarized.text4 <- data.frame(text = polarized.textx,
stringsAsFactors = FALSE) %>%
tail(22)
polarized.tokens3 <- polarized.text4 %>%
unnest_tokens("word", "text") %>%
anti_join(stop_words, by = "word")
polarized.counts.4 <- polarized.tokens3 %>%
count(word) %>%
arrange(-n) %>%
head(7)
words.second.half <- polarized.counts.4 %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Number") +
xlab("Word") +
ggtitle("Words in the Second Half of Article")
polar.tokenx <- polarized.text4 %>%
unnest_tokens("word", "text", token = "ngrams", n = 3)
polar.tokensx <- polar.tokenx %>%
count(word) %>%
arrange(-n) %>%
head(5)
phrases.second.half <- polar.tokensx %>%
ggplot(aes(x = reorder(word, n),
y = n)) +
geom_col() +
coord_flip() +
ylab("Phrase") +
xlab("Number") +
ggtitle("Phrases in the Second Half of Article")
grid.arrange(words.first.half, phrases.first.half, words.second.half, phrases.second.half,
words.whole.article, phrases.whole.article)
library(shiny)
library(tidyverse)
library(shiny); runApp('Downloads/Labnotes/App #1.R')
library(shiny)
library(rvest)
library(geojsonio)
setwd("~/Desktop/ Data Science/Food Deserts/nycsubwayfooddesert/CCC Data Download_20201102_160852798")
pop.data <- read.csv("Total Population.csv")
nhoods <- geojson_read(file.choose(),
what = "sp")
View(nhoods)
View(nhoods)
pop.data <- read.csv(file.choose())
med.income <- read.csv(file.choose())
nhoods2 <- nhoods
